{"cells":[{"cell_type":"markdown","metadata":{"id":"fviFgNgO2a6t"},"source":["# xAI Interface: Grok Knowledge Base from Google Folder\n","The Jupyter Notebook is designed as a tool for managing and interacting with a vectorized knowledge base and AI systems. Below is a high-level summary of its contents and instructions on how to use it:\n","\n","## Notebook Overview\n","1.\tTitle: xAI Interface\n","-\tFocus: Managing and interacting with a vectorized knowledge base and AI systems.\n","2.\tSections:\n","-\tIntroduction: Provides an overview of the notebookâ€™s functionality.\n","-\tSetup Modules:\n"," -\tInstall necessary Python packages (faiss-cpu, faiss-gpu, etc.).\n"," -\tLoad required modules.\n","-\tGoogle Drive Integration:\n"," -\tMount Google Drive for data input/output.\n","-\tEnvironment Variable Loading:\n"," -\tLoad variables from an .env file.\n","-\tTPU Setup:\n"," -\tCheck for TPU (Tensor Processing Unit) availability for accelerated computation.\n","-\tVectorization Process:\n"," -\tProcesses documents into a vectorized format.\n","-\tValidation of Outputs:\n"," -\tVerify that the vectorized outputs are correctly generated.\n","-\tInteraction with xAI and Vector Knowledgebase:\n"," -\tProvides a user interface for interacting with the vectorized database using AI-powered tools.\n","\n","## Instructions to Use\n","1.\tInstall Dependencies:\n","-\tRun the provided code cell for package installation to ensure all necessary libraries are available.\n","-\tMake sure to have appropriate permissions to install packages if using a shared environment.\n","2.\tMount Google Drive:\n","-\tRun the relevant cell to authenticate and mount Google Drive, allowing access to input/output data.\n","3.\tSet Up Environment Variables:\n","-\tUpdate the .env file path to match your file structure.\n","-\tEnsure the required variables are defined in your .env file.\n","4.\tCheck TPU Availability (Optional):\n","-\tIf using TPU for computation, run the TPU setup cell to verify availability.\n","5.\tProcess Data:\n","-\tConfigure the document folder to be vectorized.\n","-\tRun the vectorization step to process documents and store the results.\n","6.\tValidate Results:\n","-\tUse the validation cell to ensure that the outputs of the vectorization process are as expected.\n","7.\tInteract with the Knowledge Base:\n","-\tA user-friendly interface is provided for querying the vectorized database.\n","-\tRun the interaction cells to start communicating with the xAI system.\n","8.\tReview Outputs:\n","-\tExamine results displayed in the interface or saved to your Google Drive for further analysis."]},{"cell_type":"markdown","metadata":{"id":"oPiQPpUDIHaF"},"source":["## Load modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayrM5A5rPXkf","cellView":"form"},"outputs":[],"source":["# @title\n","# Install necessary packages\n","!pip install faiss-cpu faiss-gpu joblib google-api-python-client ipywidgets python-dotenv tqdm ray[default] psutil pyyaml typing-extensions --quiet"]},{"cell_type":"markdown","metadata":{"id":"PCfCb_37KeB9"},"source":["## Mount Google drive for input and output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEnWdplzNaqE","cellView":"form"},"outputs":[],"source":["# @title\n","from google.colab import auth, drive\n","from googleapiclient.discovery import build\n","\n","# Authenticate user and set up Google Drive service\n","auth.authenticate_user()\n","drive_service = build('drive', 'v3')\n","# Mount Google Drive for file operations\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"qCpY_IuYVlJh"},"source":["# Load Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Kd8p0NiG-YT","cellView":"form"},"outputs":[],"source":["# @title\n","from dotenv import load_dotenv\n","import os\n","\n","# Load environment variables from the .env file\n","env_file_path = \"/content/drive/MyDrive/Colab Notebooks/.env\"  # Update the path to the location of your .env file\n","load_dotenv(env_file_path)\n","\n","# Access the environment variables\n","grok_api_key = os.getenv(\"GROK_API_KEY\")\n","vector_db_dir = os.getenv(\"VECTOR_DB_DIR\") # does not appear to be used\n","vectors_joblib = os.getenv(\"VECTORS_JOBLIB\") # does not appear to be used\n","vectorizer_joblib = os.getenv(\"VECTORIZER_JOBLIB\") # does not appear to be used\n","hashes_joblib = os.getenv(\"HASHES_JOBLIB\") # does not appear to be used\n","FOLDER_ID = os.getenv(\"FOLDER_ID\")\n","SAVE_DIR = os.getenv(\"SAVE_DIR\")\n","MODEL = os.getenv(\"GROK_MODEL\")\n","TEMP = os.getenv(\"GROK_TEMP\")\n","\n","# Example usage: print the values\n","print(f\"GROK_API_KEY: {grok_api_key}\")\n","print(f\"VECTOR_DB_DIR: {vector_db_dir}\")\n","print(f\"VECTORS_JOBLIB: {vectors_joblib}\")\n","print(f\"VECTORIZER_JOBLIB: {vectorizer_joblib}\")\n","print(f\"HASHES_JOBLIB: {hashes_joblib}\")\n","print(f\"FOLDER_ID: {FOLDER_ID}\")\n","print(f\"SAVE_DIR: {SAVE_DIR}\")\n","print(f\"GROK_MODEL: {MODEL}\")\n","print(f\"GROK_TEMP: {TEMP}\")"]},{"cell_type":"markdown","metadata":{"id":"MfQ5BTEc2JJL"},"source":["## Check TPU availability"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QrmNqkMK5Vs9","cellView":"form"},"outputs":[],"source":["# @title\n","import tensorflow as tf\n","\n","try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    print('Running on TPU ', tpu.master())\n","except ValueError:\n","    print('TPU not found')"]},{"cell_type":"markdown","metadata":{"id":"0CIY6JQ42WTh"},"source":["## Vectorize the folder defined in this step"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_RBrCh1BPiSM","cellView":"form"},"outputs":[],"source":["# @title\n","\"\"\"\n","Enhanced Google Drive Document Processor with Parallel Processing and Improved Resumability\n","\"\"\"\n","\n","from typing import List, Dict, Any, Optional, Set, Tuple\n","from contextlib import contextmanager\n","from dataclasses import dataclass\n","import signal\n","from datetime import datetime, timedelta\n","from scipy import sparse\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import ray\n","from google.colab import auth, drive\n","from googleapiclient.discovery import build\n","from googleapiclient.http import MediaIoBaseDownload\n","from googleapiclient.errors import HttpError\n","import mimetypes\n","import pickle\n","import numpy as np\n","import psutil\n","from pathlib import Path\n","import yaml\n","import zlib\n","from typing_extensions import Protocol\n","import faiss\n","import json\n","import logging\n","import multiprocessing\n","import os\n","import sys\n","import time\n","import io\n","import errno\n","import fcntl\n","\n","# Custom exceptions\n","class DocumentProcessingError(Exception):\n","    \"\"\"Base exception for document processing errors\"\"\"\n","    pass\n","\n","class RateLimitError(DocumentProcessingError):\n","    \"\"\"Raised when API rate limits are hit\"\"\"\n","    pass\n","\n","class ConfigurationError(Exception):\n","    \"\"\"Raised for configuration-related errors\"\"\"\n","    pass\n","\n","@dataclass\n","class ProcessingMetrics:\n","    \"\"\"Metrics for monitoring processing progress\"\"\"\n","    start_time: float\n","    processed_files: int = 0\n","    failed_files: int = 0\n","    total_bytes: int = 0\n","    vectorization_time: float = 0\n","    processing_time: float = 0\n","    memory_usage: float = 0\n","\n","    @property\n","    def duration(self) -> float:\n","        return time.time() - self.start_time\n","\n","    def to_dict(self) -> Dict[str, Any]:\n","        return {\n","            'duration': self.duration,\n","            'processed_files': self.processed_files,\n","            'failed_files': self.failed_files,\n","            'total_bytes': self.total_bytes,\n","            'vectorization_time': self.vectorization_time,\n","            'processing_time': self.processing_time,\n","            'memory_usage_mb': self.memory_usage / (1024 * 1024)\n","        }\n","\n","class Logger:\n","    \"\"\"Enhanced logging functionality with reduced duplicates and cleaner output\"\"\"\n","    def __init__(self, save_dir: str, level: str = 'INFO'):\n","        self.logger = logging.getLogger('DocumentProcessor')\n","        self.logger.setLevel(getattr(logging, level.upper()))\n","\n","        # Remove any existing handlers to prevent duplicate logging\n","        for handler in self.logger.handlers[:]:\n","            self.logger.removeHandler(handler)\n","\n","        # Clear root logger handlers to prevent duplicate logs\n","        root_logger = logging.getLogger()\n","        for handler in root_logger.handlers[:]:\n","            root_logger.removeHandler(handler)\n","\n","        # Create log directory if it doesn't exist\n","        log_dir = Path(save_dir)\n","        log_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Set up file handler\n","        log_file = log_dir / 'document_processor.log'\n","        file_handler = logging.FileHandler(log_file)\n","        file_handler.setFormatter(\n","            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s',\n","                            datefmt='%Y-%m-%d %H:%M:%S')\n","        )\n","        self.logger.addHandler(file_handler)\n","\n","        # Set up console handler\n","        console_handler = logging.StreamHandler()\n","        console_handler.setFormatter(\n","            logging.Formatter('%(asctime)s - %(levelname)s - %(message)s',\n","                            datefmt='%Y-%m-%d %H:%M:%S')\n","        )\n","        self.logger.addHandler(console_handler)\n","\n","        # Prevent propagation to avoid duplicate logs\n","        self.logger.propagate = False\n","\n","        # Track start time for relative timestamps\n","        self.start_time = time.time()\n","\n","    def format_time_delta(self, seconds: float) -> str:\n","        \"\"\"Format time delta in a human-readable format\"\"\"\n","        hours = int(seconds // 3600)\n","        minutes = int((seconds % 3600) // 60)\n","        secs = int(seconds % 60)\n","        if hours > 0:\n","            return f\"{hours}h {minutes}m {secs}s\"\n","        elif minutes > 0:\n","            return f\"{minutes}m {secs}s\"\n","        return f\"{secs}s\"\n","\n","    def add_timing(self, msg: str) -> str:\n","        \"\"\"Add elapsed time to message\"\"\"\n","        elapsed = time.time() - self.start_time\n","        time_str = self.format_time_delta(elapsed)\n","        return f\"[{time_str}] {msg}\"\n","\n","    def info(self, msg: str):\n","        \"\"\"Log info message with elapsed time\"\"\"\n","        self.logger.info(self.add_timing(msg))\n","\n","    def error(self, msg: str):\n","        \"\"\"Log error message with elapsed time\"\"\"\n","        self.logger.error(self.add_timing(msg))\n","\n","    def warning(self, msg: str):\n","        \"\"\"Log warning message with elapsed time\"\"\"\n","        self.logger.warning(self.add_timing(msg))\n","\n","    def debug(self, msg: str):\n","        \"\"\"Log debug message with elapsed time\"\"\"\n","        self.logger.debug(self.add_timing(msg))\n","\n","    def get_log_file_path(self) -> Path:\n","        \"\"\"Get the path to the current log file\"\"\"\n","        for handler in self.logger.handlers:\n","            if isinstance(handler, logging.FileHandler):\n","                return Path(handler.baseFilename)\n","        return None\n","\n","class Config:\n","    \"\"\"Enhanced configuration management\"\"\"\n","    def __init__(self, config_path: Optional[str] = None):\n","        self._defaults = {\n","            'PROCESS_ZIP_FILES': True,\n","            'BATCH_SIZE': 5,\n","            'FILE_WORKERS': multiprocessing.cpu_count() * 2,\n","            'VECTORIZE_WORKERS': multiprocessing.cpu_count(),\n","            'LOCK_TIMEOUT': 60,\n","            'CHUNK_SIZE': 1000,\n","            'MAX_RETRIES': 3,\n","            'SAVE_INTERVAL': 300,\n","            'CHECKPOINT_INTERVAL': 50,\n","            'MAX_FILE_SIZE': 100 * 1024 * 1024,  # 100MB\n","            'MAX_CACHE_SIZE': 1000,\n","            'MAX_RECURSION_DEPTH': 10,\n","            'COMPRESSION_LEVEL': 6,\n","            'LOG_LEVEL': 'INFO',\n","            'MONITORING_INTERVAL': 60,\n","            'VECTOR_DIMENSION': 512,\n","            'MIN_DF': 2,\n","            'MAX_DF': 0.95,\n","            'RETRY_DELAY_BASE': 2,\n","            'MAX_RETRY_DELAY': 60,\n","            'BATCH_TIMEOUT': 3600,\n","            'API_QUOTA_LIMIT': 10000,\n","            'API_QUOTA_WINDOW': 100,\n","            'MEMORY_LIMIT': 0.9  # 80% of available RAM\n","        }\n","\n","        self._config = self._defaults.copy()\n","        if config_path:\n","            self._load_config(config_path)\n","        self._load_env_vars()\n","        self._validate_config()\n","\n","    def _load_config(self, config_path: str):\n","        try:\n","            path = Path(config_path)\n","            if not path.exists():\n","                raise ConfigurationError(f\"Config file not found: {config_path}\")\n","\n","            with open(path, 'r') as f:\n","                custom_config = yaml.safe_load(f)\n","\n","            if not isinstance(custom_config, dict):\n","                raise ConfigurationError(\"Invalid config format\")\n","\n","            for key in custom_config:\n","                if key in self._defaults:\n","                    self._config[key] = custom_config[key]\n","                else:\n","                    raise ConfigurationError(f\"Unknown configuration key: {key}\")\n","\n","        except yaml.YAMLError as e:\n","            raise ConfigurationError(f\"Error parsing config file: {e}\")\n","        except Exception as e:\n","            raise ConfigurationError(f\"Error loading config: {e}\")\n","\n","    def _load_env_vars(self):\n","        for key in self._config:\n","            env_key = f\"DOC_PROCESSOR_{key}\"\n","            env_val = os.getenv(env_key)\n","\n","            if env_val is not None:\n","                try:\n","                    default_type = type(self._defaults[key])\n","                    if default_type == bool:\n","                        self._config[key] = env_val.lower() in ('true', '1', 'yes')\n","                    else:\n","                        self._config[key] = default_type(env_val)\n","                except ValueError as e:\n","                    raise ConfigurationError(f\"Invalid environment variable value for {key}: {e}\")\n","\n","    def _validate_config(self):\n","        validators = {\n","            'BATCH_SIZE': lambda x: 0 < x <= 1000,\n","            'FILE_WORKERS': lambda x: 0 < x <= multiprocessing.cpu_count() * 4,\n","            'VECTORIZE_WORKERS': lambda x: 0 < x <= multiprocessing.cpu_count() * 4,\n","            'MAX_FILE_SIZE': lambda x: 0 < x <= 1024 * 1024 * 1024,\n","            'MAX_CACHE_SIZE': lambda x: x > 0,\n","            'MAX_RECURSION_DEPTH': lambda x: 0 < x <= 20,\n","            'COMPRESSION_LEVEL': lambda x: 0 <= x <= 9,\n","            'MEMORY_LIMIT': lambda x: 0 < x <= 0.95\n","        }\n","\n","        for key, validator in validators.items():\n","            if not validator(self._config[key]):\n","                raise ConfigurationError(f\"Invalid configuration for {key}\")\n","\n","    def __getattr__(self, name: str) -> Any:\n","        if name in self.__dict__:\n","            return self.__dict__[name]\n","        if '_config' in self.__dict__ and name in self.__dict__['_config']:\n","            return self.__dict__['_config'][name]\n","        raise AttributeError(f\"Configuration has no attribute '{name}'\")\n","\n","    def validate_memory_usage(self) -> bool:\n","        try:\n","            memory_percent = psutil.Process().memory_percent()\n","            return memory_percent / 100 < self.MEMORY_LIMIT\n","        except Exception:\n","            return True\n","\n","class ProcessingProgress:\n","    \"\"\"Enhanced progress tracking and resumption management\"\"\"\n","    def __init__(self, save_dir: str, logger: Logger):\n","        self.save_dir = Path(save_dir)\n","        self.progress_file = self.save_dir / 'progress.json'\n","        self.logger = logger\n","        self.last_status_time = datetime.now()\n","        self.status_interval = timedelta(minutes=5)\n","\n","        # Track skipped files and reasons\n","        self.skipped_files: Dict[str, Set[str]] = {\n","            'unsupported_type': set(),\n","            'too_large': set(),\n","            'failed_conversion': set(),\n","            'rate_limited': set(),\n","            'permission_denied': set(),\n","            'already_processed': set()\n","        }\n","\n","        # Track current batch progress\n","        self.current_batch: Optional[Dict[str, Any]] = None\n","        self.processing_state = None  # Will be set by processor\n","        self.load_progress()\n","\n","    def load_progress(self):\n","        \"\"\"Load saved progress with validation\"\"\"\n","        try:\n","            if self.progress_file.exists():\n","                with open(self.progress_file, 'r') as f:\n","                    data = json.load(f)\n","\n","                # Validate structure\n","                required_keys = {'last_folder_id', 'last_batch_start', 'total_processed',\n","                               'skipped_files', 'current_batch'}\n","                if all(key in data for key in required_keys):\n","                    self.current_batch = data.get('current_batch')\n","                    # Convert sets from lists\n","                    self.skipped_files = {\n","                        category: set(files)\n","                        for category, files in data.get('skipped_files', {}).items()\n","                    }\n","                    self.logger.info(f\"Loaded progress: {data['total_processed']} files processed\")\n","                    return\n","\n","            self.logger.warning(\"No valid progress file found, starting fresh\")\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error loading progress: {e}\")\n","            self.logger.warning(\"Starting fresh due to progress load error\")\n","\n","    def save_progress(self, force: bool = False):\n","        \"\"\"Save progress with atomic write\"\"\"\n","        try:\n","            current_time = datetime.now()\n","\n","            # Print status every 5 minutes\n","            if current_time - self.last_status_time >= self.status_interval or force:\n","                total_skipped = sum(len(files) for files in self.skipped_files.values())\n","                self.logger.info(\n","                    f\"Status: Processed={len(self.processing_state.processed_files)}, \"\n","                    f\"Failed={len(self.processing_state.failed_files)}, \"\n","                    f\"Skipped={total_skipped}\"\n","                )\n","                self.last_status_time = current_time\n","\n","            progress_data = {\n","                'timestamp': current_time.isoformat(),\n","                'last_folder_id': self.current_batch.get('folder_id') if self.current_batch else None,\n","                'last_batch_start': self.current_batch.get('start_index') if self.current_batch else 0,\n","                'total_processed': len(self.processing_state.processed_files),\n","                'skipped_files': {\n","                    category: list(files) for category, files in self.skipped_files.items()\n","                },\n","                'current_batch': self.current_batch\n","            }\n","\n","            # Atomic write using temporary file\n","            temp_file = self.progress_file.with_suffix('.tmp')\n","            with open(temp_file, 'w') as f:\n","                json.dump(progress_data, f, indent=2)\n","            temp_file.rename(self.progress_file)\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error saving progress: {e}\")\n","\n","    def mark_skipped(self, file_id: str, reason: str):\n","        \"\"\"Track skipped files by reason\"\"\"\n","        if reason in self.skipped_files:\n","            self.skipped_files[reason].add(file_id)\n","        else:\n","            self.logger.warning(f\"Unknown skip reason: {reason}\")\n","\n","class GracefulShutdown:\n","    \"\"\"\n","    Enhanced shutdown handler with robust cleanup and detailed logging.\n","    Handles both graceful and forced shutdowns, ensuring data consistency.\n","    \"\"\"\n","    def __init__(self, processor, logger: Logger):\n","        self.processor = processor\n","        self.logger = logger\n","        self.shutdown_requested = False\n","        self.force_shutdown = False\n","        self.cleanup_started = False\n","        self.cleanup_completed = False\n","\n","        # Track component states\n","        self.component_states = {\n","            'progress_saved': False,\n","            'state_saved': False,\n","            'vectors_saved': False,\n","            'ray_shutdown': False\n","        }\n","\n","        # Register signal handlers\n","        signal.signal(signal.SIGINT, self.handle_shutdown)\n","        signal.signal(signal.SIGTERM, self.handle_shutdown)\n","        # Handle additional signals for more robust shutdown\n","        signal.signal(signal.SIGHUP, self.handle_shutdown)\n","        signal.signal(signal.SIGQUIT, self.handle_forced_shutdown)\n","\n","    def handle_shutdown(self, signum, frame):\n","        \"\"\"\n","        Handle shutdown signals with graceful fallback.\n","        First attempt graceful shutdown, then force if requested again.\n","        \"\"\"\n","        signal_names = {\n","            signal.SIGINT: 'SIGINT',\n","            signal.SIGTERM: 'SIGTERM',\n","            signal.SIGHUP: 'SIGHUP',\n","            signal.SIGQUIT: 'SIGQUIT'\n","        }\n","\n","        if self.cleanup_started:\n","            self.logger.warning(\n","                f\"Received {signal_names.get(signum, 'UNKNOWN')} during cleanup. \"\n","                \"Please wait for cleanup to complete...\"\n","            )\n","            return\n","\n","        if self.shutdown_requested:\n","            self.logger.warning(\n","                f\"Received second {signal_names.get(signum, 'UNKNOWN')}. \"\n","                \"Initiating forced shutdown...\"\n","            )\n","            self.force_shutdown = True\n","            return\n","\n","        self.logger.info(\n","            f\"Received {signal_names.get(signum, 'UNKNOWN')}. \"\n","            \"Graceful shutdown initiated...\"\n","        )\n","        self.shutdown_requested = True\n","\n","    def handle_forced_shutdown(self, signum, frame):\n","        \"\"\"Handle immediate shutdown requests (SIGQUIT)\"\"\"\n","        self.logger.warning(\"Forced shutdown requested, initiating immediate cleanup...\")\n","        self.force_shutdown = True\n","        self.shutdown_requested = True\n","\n","    def should_continue(self) -> bool:\n","        \"\"\"Check if processing should continue\"\"\"\n","        return not (self.shutdown_requested or self.force_shutdown)\n","\n","    def _save_progress(self) -> bool:\n","        \"\"\"Save processing progress with error handling\"\"\"\n","        try:\n","            self.logger.info(\"Saving processing progress...\")\n","            self.processor.progress.save_progress(force=True)\n","            self.component_states['progress_saved'] = True\n","            return True\n","        except Exception as e:\n","            self.logger.error(f\"Error saving progress: {e}\")\n","            return False\n","\n","    def _save_processing_state(self) -> bool:\n","        \"\"\"Save processing state with error handling\"\"\"\n","        try:\n","            self.logger.info(\"Saving processing state...\")\n","            self.processor.processing_state.save_state()\n","            self.component_states['state_saved'] = True\n","            return True\n","        except Exception as e:\n","            self.logger.error(f\"Error saving processing state: {e}\")\n","            return False\n","\n","    def _save_vector_store(self) -> bool:\n","        \"\"\"Save vector store state with error handling\"\"\"\n","        try:\n","            self.logger.info(\"Saving vector store state...\")\n","            self.processor.vector_store._save_state()\n","            self.component_states['vectors_saved'] = True\n","            return True\n","        except Exception as e:\n","            self.logger.error(f\"Error saving vector store: {e}\")\n","            return False\n","\n","    def _shutdown_ray(self) -> bool:\n","        \"\"\"Shutdown Ray with error handling\"\"\"\n","        try:\n","            self.logger.info(\"Shutting down Ray...\")\n","            ray.shutdown()\n","            self.component_states['ray_shutdown'] = True\n","            return True\n","        except Exception as e:\n","            self.logger.error(f\"Error shutting down Ray: {e}\")\n","            return False\n","\n","    def cleanup(self) -> bool:\n","        \"\"\"\n","        Perform complete cleanup with state tracking and error handling.\n","        Returns True if cleanup was successful, False otherwise.\n","        \"\"\"\n","        self.cleanup_started = True\n","        cleanup_successful = True\n","\n","        try:\n","            self.logger.info(\n","                f\"Starting cleanup process \"\n","                f\"({'forced' if self.force_shutdown else 'graceful'} shutdown)\"\n","            )\n","\n","            if not self.force_shutdown:\n","                # Save all states in graceful shutdown\n","                cleanup_successful &= self._save_progress()\n","                cleanup_successful &= self._save_processing_state()\n","                cleanup_successful &= self._save_vector_store()\n","            else:\n","                # In forced shutdown, just log the state\n","                self.logger.warning(\"Forced shutdown: Skipping state saves\")\n","\n","            # Always try to shutdown Ray\n","            cleanup_successful &= self._shutdown_ray()\n","\n","            # Log cleanup summary\n","            successful_components = [\n","                name for name, state in self.component_states.items()\n","                if state\n","            ]\n","            failed_components = [\n","                name for name, state in self.component_states.items()\n","                if not state\n","            ]\n","\n","            self.logger.info(\"\\nCleanup Summary:\")\n","            if successful_components:\n","                self.logger.info(\"Successfully completed:\")\n","                for component in successful_components:\n","                    self.logger.info(f\"  â€¢ {component}\")\n","\n","            if failed_components:\n","                self.logger.warning(\"Failed components:\")\n","                for component in failed_components:\n","                    self.logger.warning(f\"  â€¢ {component}\")\n","\n","            self.cleanup_completed = True\n","            return cleanup_successful\n","\n","        except Exception as e:\n","            self.logger.error(f\"Unexpected error during cleanup: {e}\")\n","            return False\n","        finally:\n","            self.logger.info(\n","                f\"Cleanup {'completed' if self.cleanup_completed else 'failed'} \"\n","                f\"({'forced' if self.force_shutdown else 'graceful'} shutdown)\"\n","            )\n","\n","class ProcessingState:\n","    \"\"\"Enhanced processing state management\"\"\"\n","    def __init__(self, save_dir: str, logger: Logger):\n","        self.save_dir = Path(save_dir)\n","        self.logger = logger\n","        self.state_file = self.save_dir / 'processing_state.pkl'\n","        self.processed_files: Set[str] = set()\n","        self.failed_files: Dict[str, str] = {}\n","        self.last_checkpoint = 0\n","        self.load_state()\n","\n","    def load_state(self):\n","        try:\n","            if self.state_file.exists():\n","                with open(self.state_file, 'rb') as f:\n","                    state = pickle.load(f)\n","\n","                required_keys = {'processed_files', 'failed_files', 'last_checkpoint'}\n","                if not all(key in state for key in required_keys):\n","                    raise ValueError(\"Invalid state file structure\")\n","\n","                self.processed_files = state['processed_files']\n","                self.failed_files = state['failed_files']\n","                self.last_checkpoint = state['last_checkpoint']\n","\n","                self.logger.info(f\"Loaded state: {len(self.processed_files)} processed, \"\n","                               f\"{len(self.failed_files)} failed\")\n","        except Exception as e:\n","            self.logger.error(f\"Error loading state: {e}\")\n","            pass\n","\n","    def save_state(self):\n","        state = {\n","            'processed_files': self.processed_files,\n","            'failed_files': self.failed_files,\n","            'last_checkpoint': self.last_checkpoint,\n","            'timestamp': datetime.now().isoformat()\n","        }\n","\n","        temp_file = self.state_file.with_suffix('.tmp')\n","        try:\n","            with open(temp_file, 'wb') as f:\n","                pickle.dump(state, f)\n","            temp_file.rename(self.state_file)\n","            self.logger.debug(\"State saved successfully\")\n","        except Exception as e:\n","            self.logger.error(f\"Error saving state: {e}\")\n","            if temp_file.exists():\n","                temp_file.unlink()\n","            raise\n","\n","    def mark_processed(self, file_id: str):\n","        self.processed_files.add(file_id)\n","        self.failed_files.pop(file_id, None)\n","\n","    def mark_failed(self, file_id: str, error: str):\n","        self.failed_files[file_id] = error\n","        self.logger.error(f\"File {file_id} failed: {error}\")\n","\n","class LRUCache:\n","    \"\"\"LRU cache implementation for file metadata\"\"\"\n","    def __init__(self, max_size: int):\n","        self.cache = {}\n","        self.max_size = max_size\n","        self.access_order = []\n","\n","    def get(self, key: str) -> Optional[Any]:\n","        if key in self.cache:\n","            self.access_order.remove(key)\n","            self.access_order.append(key)\n","            return self.cache[key]\n","        return None\n","\n","    def put(self, key: str, value: Any):\n","        if key in self.cache:\n","            self.access_order.remove(key)\n","        elif len(self.cache) >= self.max_size:\n","            lru_key = self.access_order.pop(0)\n","            self.cache.pop(lru_key)\n","\n","        self.cache[key] = value\n","        self.access_order.append(key)\n","\n","    def clear(self):\n","        self.cache.clear()\n","        self.access_order.clear()\n","\n","class FileHandler:\n","    \"\"\"Enhanced file handling with improved caching and validation\"\"\"\n","    SUPPORTED_MIMETYPES = {\n","        # Text files\n","        'text/plain': 'text',\n","        'text/html': 'text',\n","        'text/csv': 'text',\n","        'text/markdown': 'text',\n","        'text/xml': 'text',\n","        # Document files\n","        'application/pdf': 'document',\n","        'application/msword': 'document',\n","        'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'document',\n","        'application/rtf': 'document',\n","        'application/vnd.oasis.opendocument.text': 'document',\n","        # Spreadsheets\n","        'application/vnd.ms-excel': 'spreadsheet',\n","        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'spreadsheet',\n","        # Google workspace\n","        'application/vnd.google-apps.document': 'google_doc',\n","        'application/vnd.google-apps.spreadsheet': 'google_sheet',\n","        'application/vnd.google-apps.presentation': 'google_slide',\n","        # Archives\n","        'application/zip': 'archive',\n","        'application/x-rar-compressed': 'archive',\n","        'application/x-7z-compressed': 'archive',\n","        # Code files\n","        'text/x-python': 'code',\n","        'application/javascript': 'code',\n","        'text/x-java': 'code',\n","        'text/x-c': 'code',\n","        'text/x-cpp': 'code'\n","    }\n","\n","    def __init__(self, service, processing_state, config, logger):\n","        self.service = service\n","        self.processing_state = processing_state\n","        self.config = config\n","        self.logger = logger\n","        self._file_cache = LRUCache(config.MAX_CACHE_SIZE)\n","        self.metrics = ProcessingMetrics(start_time=time.time())\n","        self._request_timestamps = []\n","        self._last_rate_limit_check = time.time()\n","\n","    def _enforce_rate_limits(self):\n","        \"\"\"Enforce API rate limits\"\"\"\n","        current_time = time.time()\n","        window_start = current_time - self.config.API_QUOTA_WINDOW\n","\n","        # Remove old timestamps\n","        self._request_timestamps = [ts for ts in self._request_timestamps if ts > window_start]\n","\n","        # Check if we're over the limit\n","        if len(self._request_timestamps) >= self.config.API_QUOTA_LIMIT:\n","            sleep_time = min(window_start + self.config.API_QUOTA_WINDOW - current_time,\n","                           self.config.MAX_RETRY_DELAY)\n","            self.logger.warning(f\"API quota limit reached, waiting {sleep_time:.2f}s\")\n","            time.sleep(sleep_time)\n","\n","    def list_files_recursive(self, folder_id: str) -> List[Dict[str, Any]]:\n","        \"\"\"List all files using iteration instead of recursion\"\"\"\n","        files = []\n","        folders_to_process = [(folder_id, 0)]  # (folder_id, depth)\n","        processed_folders = set()\n","\n","        start_time = time.time()\n","        total_files = 0\n","        total_folders = 0\n","\n","        while folders_to_process:\n","            if time.time() - start_time > self.config.BATCH_TIMEOUT:\n","                self.logger.warning(\"Batch timeout reached, stopping folder traversal\")\n","                break\n","\n","            if not self.config.validate_memory_usage():\n","                self.logger.warning(\"Memory usage exceeded limit, stopping folder traversal\")\n","                break\n","\n","            current_folder, depth = folders_to_process.pop(0)\n","\n","            if depth > self.config.MAX_RECURSION_DEPTH or current_folder in processed_folders:\n","                continue\n","\n","            processed_folders.add(current_folder)\n","            total_folders += 1\n","\n","            try:\n","                cached_files = self._file_cache.get(current_folder)\n","                if cached_files is not None:\n","                    files.extend(cached_files)\n","                    total_files += len(cached_files)\n","                    continue\n","\n","                folder_files = []\n","                page_token = None\n","\n","                while True:\n","                    self._enforce_rate_limits()\n","\n","                    try:\n","                        results = self.service.files().list(\n","                            q=f\"'{current_folder}' in parents and trashed=false\",\n","                            spaces='drive',\n","                            fields='nextPageToken, files(id, name, mimeType, size, modifiedTime)',\n","                            pageToken=page_token,\n","                            pageSize=1000\n","                        ).execute()\n","\n","                        self._request_timestamps.append(time.time())\n","\n","                        for file in results.get('files', []):\n","                            if file['mimeType'] == 'application/vnd.google-apps.folder':\n","                                if depth + 1 <= self.config.MAX_RECURSION_DEPTH:\n","                                    folders_to_process.append((file['id'], depth + 1))\n","                            elif file['id'] not in self.processing_state.processed_files:\n","                                folder_files.append(file)\n","                                total_files += 1\n","\n","                        files.extend(folder_files)\n","                        self._file_cache.put(current_folder, folder_files)\n","\n","                        page_token = results.get('nextPageToken')\n","                        if not page_token:\n","                            break\n","\n","                    except HttpError as e:\n","                        if e.resp.status == 429:  # Rate limit\n","                            wait_time = min(self.config.RETRY_DELAY_BASE ** depth,\n","                                         self.config.MAX_RETRY_DELAY)\n","                            self.logger.warning(f\"Rate limit hit, waiting {wait_time}s\")\n","                            time.sleep(wait_time)\n","                            continue\n","                        raise\n","\n","            except Exception as e:\n","                self.logger.error(f\"Error processing folder {current_folder}: {e}\")\n","\n","        duration = time.time() - start_time\n","        self.logger.info(f\"Folder traversal completed in {duration:.2f}s: \"\n","                        f\"processed {total_folders} folders, found {total_files} files\")\n","\n","        return files\n","\n","    def get_mime_type(self, file_id: str) -> Optional[str]:\n","        \"\"\"Get mime type with fallback to extension-based detection\"\"\"\n","        try:\n","            self._enforce_rate_limits()\n","\n","            file = self.service.files().get(\n","                fileId=file_id,\n","                fields='mimeType, name'\n","            ).execute()\n","\n","            self._request_timestamps.append(time.time())\n","\n","            mime_type = file.get('mimeType')\n","            if mime_type not in self.SUPPORTED_MIMETYPES:\n","                ext = os.path.splitext(file.get('name', ''))[1].lower()\n","                guessed_type = mimetypes.guess_type(f\"file{ext}\")[0]\n","                if guessed_type in self.SUPPORTED_MIMETYPES:\n","                    return guessed_type\n","            return mime_type\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error getting mime type for {file_id}: {e}\")\n","            return None\n","\n","    def convert_and_extract_text(self, file_id: str) -> Optional[str]:\n","        \"\"\"Convert and extract text content from a file with retries\"\"\"\n","        if file_id in self.processing_state.processed_files:\n","            return None\n","\n","        try:\n","            mime_type = self.get_mime_type(file_id)\n","            if not mime_type or mime_type not in self.SUPPORTED_MIMETYPES:\n","                self.processing_state.mark_failed(file_id, f\"Unsupported mime type: {mime_type}\")\n","                return None\n","\n","            self._enforce_rate_limits()\n","            file = self.service.files().get(fileId=file_id, fields='size').execute()\n","            self._request_timestamps.append(time.time())\n","\n","            if int(file.get('size', 0)) > self.config.MAX_FILE_SIZE:\n","                self.processing_state.mark_failed(file_id, \"File too large\")\n","                return None\n","\n","            for attempt in range(self.config.MAX_RETRIES):\n","                try:\n","                    if mime_type.startswith('application/vnd.google-apps'):\n","                        request = self.service.files().export_media(\n","                            fileId=file_id,\n","                            mimeType='text/plain'\n","                        )\n","                    else:\n","                        request = self.service.files().get_media(fileId=file_id)\n","\n","                    fh = io.BytesIO()\n","                    downloader = MediaIoBaseDownload(fh, request)\n","                    done = False\n","\n","                    while not done:\n","                        self._enforce_rate_limits()\n","                        _, done = downloader.next_chunk()\n","\n","                    text = fh.getvalue().decode('utf-8', errors='ignore')\n","\n","                    self.metrics.total_bytes += len(text.encode('utf-8'))\n","                    self.metrics.processed_files += 1\n","\n","                    self.processing_state.mark_processed(file_id)\n","                    return text\n","\n","                except HttpError as e:\n","                    if e.resp.status == 429:  # Rate limit\n","                        wait_time = min(\n","                            self.config.RETRY_DELAY_BASE ** attempt,\n","                            self.config.MAX_RETRY_DELAY\n","                        )\n","                        self.logger.warning(f\"Rate limit hit, waiting {wait_time}s\")\n","                        time.sleep(wait_time)\n","                    else:\n","                        raise\n","                except Exception as e:\n","                    if attempt < self.config.MAX_RETRIES - 1:\n","                        self.logger.warning(f\"Retry {attempt + 1} for file {file_id}: {e}\")\n","                        time.sleep(self.config.RETRY_DELAY_BASE ** attempt)\n","                        continue\n","                    raise\n","\n","        except Exception as e:\n","            error_msg = f\"Error extracting text: {str(e)}\"\n","            self.processing_state.mark_failed(file_id, error_msg)\n","            self.metrics.failed_files += 1\n","            return None\n","\n","    def clear_cache(self):\n","        \"\"\"Clear the file cache\"\"\"\n","        self._file_cache.clear()\n","        self.logger.debug(\"File cache cleared\")\n","\n","class VectorStore:\n","    \"\"\"Enhanced vector storage using FAISS for indexing with better synchronization\"\"\"\n","    def __init__(self, save_dir: str, config: Config, logger: Logger):\n","        self.save_dir = Path(save_dir)\n","        self.config = config\n","        self.logger = logger\n","        self.index_path = self.save_dir / 'faiss_index.index'\n","        self.metadata_path = self.save_dir / 'metadata.json'\n","        self.dimension = self.config.VECTOR_DIMENSION\n","        self._load_state()\n","\n","    def _load_state(self):\n","        \"\"\"Load or create index and metadata with validation\"\"\"\n","        try:\n","            if self.index_path.exists() and self.metadata_path.exists():\n","                self.index = faiss.read_index(str(self.index_path))\n","                with open(self.metadata_path) as f:\n","                    self.metadata = json.load(f)\n","\n","                # Validate index and metadata consistency\n","                if self.index.ntotal != len(self.metadata.get('vector_map', {})):\n","                    self.logger.warning(\n","                        f\"Index/metadata mismatch detected. Rebuilding... \"\n","                        f\"(vectors: {self.index.ntotal}, metadata: {len(self.metadata.get('vector_map', {}))})\"\n","                    )\n","                    self._rebuild_index()\n","            else:\n","                self.index = faiss.IndexFlatL2(self.dimension)\n","                self.metadata = {'vector_map': {}, 'last_update': None}\n","                self._save_state()\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error loading vector store state: {e}\")\n","            self.index = faiss.IndexFlatL2(self.dimension)\n","            self.metadata = {'vector_map': {}, 'last_update': None}\n","\n","    def _rebuild_index(self):\n","        \"\"\"Rebuild index from scratch using metadata\"\"\"\n","        new_index = faiss.IndexFlatL2(self.dimension)\n","        new_metadata = {'vector_map': {}, 'last_update': datetime.now().isoformat()}\n","\n","        try:\n","            # Collect valid vectors\n","            valid_vectors = []\n","            valid_file_ids = []\n","\n","            for file_id, meta in self.metadata['vector_map'].items():\n","                idx = meta['index']\n","                if idx < self.index.ntotal:\n","                    vector = self.index.reconstruct(idx)\n","                    valid_vectors.append(vector)\n","                    valid_file_ids.append(file_id)\n","\n","            if valid_vectors:\n","                # Convert to numpy array\n","                vectors = np.vstack(valid_vectors)\n","                # Add to new index\n","                new_index.add(vectors)\n","\n","                # Update metadata\n","                for i, file_id in enumerate(valid_file_ids):\n","                    new_metadata['vector_map'][file_id] = {\n","                        'index': i,\n","                        'timestamp': datetime.now().isoformat()\n","                    }\n","\n","            # Save rebuilt state\n","            self.index = new_index\n","            self.metadata = new_metadata\n","            self._save_state()\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error rebuilding index: {e}\")\n","            # Create fresh index\n","            self.index = faiss.IndexFlatL2(self.dimension)\n","            self.metadata = {'vector_map': {}, 'last_update': datetime.now().isoformat()}\n","            self._save_state()\n","\n","    def _save_state(self):\n","        \"\"\"Save both index and metadata atomically\"\"\"\n","        temp_index = self.index_path.with_suffix('.tmp')\n","        temp_metadata = self.metadata_path.with_suffix('.tmp')\n","\n","        try:\n","            # Save index\n","            faiss.write_index(self.index, str(temp_index))\n","\n","            # Save metadata\n","            with open(temp_metadata, 'w') as f:\n","                json.dump(self.metadata, f, indent=2)\n","\n","            # Atomic rename\n","            os.replace(temp_index, self.index_path)\n","            os.replace(temp_metadata, self.metadata_path)\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error saving vector store state: {e}\")\n","            # Clean up temp files\n","            if temp_index.exists():\n","                temp_index.unlink()\n","            if temp_metadata.exists():\n","                temp_metadata.unlink()\n","            raise\n","\n","    def save_vectors(self, vectors: np.ndarray, file_ids: List[str]) -> bool:\n","        \"\"\"Save vectors to index with validation and atomic updates\"\"\"\n","        if len(vectors) != len(file_ids):\n","            self.logger.error(f\"Vector count ({len(vectors)}) doesn't match file_ids count ({len(file_ids)})\")\n","            return False\n","\n","        try:\n","            # Filter out already existing files\n","            new_vectors = []\n","            new_file_ids = []\n","            for vec, file_id in zip(vectors, file_ids):\n","                if file_id not in self.metadata['vector_map']:\n","                    new_vectors.append(vec)\n","                    new_file_ids.append(file_id)\n","\n","            if not new_vectors:\n","                return True  # Nothing new to add\n","\n","            # Stack vectors\n","            vector_array = np.vstack(new_vectors)\n","\n","            # Add to index\n","            start_idx = self.index.ntotal\n","            self.index.add(vector_array)\n","\n","            # Update metadata\n","            for i, file_id in enumerate(new_file_ids):\n","                self.metadata['vector_map'][file_id] = {\n","                    'index': start_idx + i,\n","                    'timestamp': datetime.now().isoformat()\n","                }\n","\n","            # Save state\n","            self.metadata['last_update'] = datetime.now().isoformat()\n","            self._save_state()\n","\n","            self.logger.info(f\"Added {len(new_vectors)} new vectors to index\")\n","            return True\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error saving vectors: {e}\")\n","            self._load_state()  # Reload previous state\n","            return False\n","\n","    def search(self, query_vector: np.ndarray, k: int = 10) -> List[Tuple[str, float]]:\n","        \"\"\"Search for k nearest neighbors with validation\"\"\"\n","        try:\n","            if query_vector.shape[0] != self.dimension:\n","                self.logger.error(f\"Invalid query vector dimension: {query_vector.shape[0]}, expected: {self.dimension}\")\n","                return []\n","\n","            distances, indices = self.index.search(query_vector.reshape(1, -1), min(k, self.index.ntotal))\n","\n","            results = []\n","            for idx, dist in zip(indices[0], distances[0]):\n","                # Find matching file_id\n","                for file_id, meta in self.metadata['vector_map'].items():\n","                    if meta['index'] == idx:\n","                        similarity = 1 - (dist / 2)  # Convert L2 distance to similarity\n","                        results.append((file_id, similarity))\n","                        break\n","\n","            return sorted(results, key=lambda x: x[1], reverse=True)\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error during search: {e}\")\n","            return []\n","\n","    def get_vector(self, file_id: str) -> Optional[np.ndarray]:\n","        \"\"\"Retrieve vector for a specific file\"\"\"\n","        try:\n","            meta = self.metadata['vector_map'].get(file_id)\n","            if not meta:\n","                return None\n","\n","            idx = meta['index']\n","            if idx >= self.index.ntotal:\n","                return None\n","\n","            return self.index.reconstruct(idx)\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error retrieving vector for {file_id}: {e}\")\n","            return None\n","\n","    def remove_vectors(self, file_ids: List[str]) -> bool:\n","        \"\"\"Remove vectors for specified files\"\"\"\n","        try:\n","            # Collect remaining vectors and their file IDs\n","            keep_vectors = []\n","            keep_file_ids = []\n","\n","            for file_id, meta in self.metadata['vector_map'].items():\n","                if file_id not in file_ids:\n","                    vector = self.index.reconstruct(meta['index'])\n","                    keep_vectors.append(vector)\n","                    keep_file_ids.append(file_id)\n","\n","            # Create new index\n","            new_index = faiss.IndexFlatL2(self.dimension)\n","            new_metadata = {'vector_map': {}, 'last_update': datetime.now().isoformat()}\n","\n","            if keep_vectors:\n","                # Add remaining vectors\n","                vectors = np.vstack(keep_vectors)\n","                new_index.add(vectors)\n","\n","                # Update metadata\n","                for i, file_id in enumerate(keep_file_ids):\n","                    new_metadata['vector_map'][file_id] = {\n","                        'index': i,\n","                        'timestamp': datetime.now().isoformat()\n","                    }\n","\n","            # Save new state\n","            self.index = new_index\n","            self.metadata = new_metadata\n","            self._save_state()\n","\n","            return True\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error removing vectors: {e}\")\n","            self._load_state()  # Restore previous state\n","            return False\n","\n","    def clear(self) -> bool:\n","        \"\"\"Clear all vectors and metadata\"\"\"\n","        try:\n","            self.index = faiss.IndexFlatL2(self.dimension)\n","            self.metadata = {'vector_map': {}, 'last_update': datetime.now().isoformat()}\n","            self._save_state()\n","            return True\n","        except Exception as e:\n","            self.logger.error(f\"Error clearing vector store: {e}\")\n","            return False\n","\n","@ray.remote\n","def process_file_chunk(service, files: List[Dict[str, Any]], processing_state: ProcessingState,\n","                      config: Config, logger: Logger) -> List[Dict[str, Any]]:\n","    \"\"\"Process a chunk of files in parallel\"\"\"\n","    results = []\n","    handler = FileHandler(service, processing_state, config, logger)\n","\n","    for file in files:\n","        try:\n","            text = handler.convert_and_extract_text(file['id'])\n","            if text:\n","                results.append({\n","                    'id': file['id'],\n","                    'name': file['name'],\n","                    'text': text,\n","                    'size': len(text.encode('utf-8'))\n","                })\n","        except Exception as e:\n","            logger.error(f\"Error processing file {file['name']}: {e}\")\n","\n","    return results\n","\n","@ray.remote\n","class RayVectorizer:\n","    \"\"\"Distributed vectorization with improved error handling\"\"\"\n","    def __init__(self, config: Config):\n","        self.config = config\n","        self.vectorizer = TfidfVectorizer(\n","            max_features=config.VECTOR_DIMENSION,\n","            ngram_range=(1, 2),\n","            strip_accents='unicode',\n","            analyzer='word',\n","            min_df=config.MIN_DF,\n","            max_df=config.MAX_DF,\n","            stop_words='english'\n","        )\n","        self.is_fitted = False\n","\n","    def fit_transform(self, texts: List[str]) -> np.ndarray:\n","        try:\n","            if not texts:\n","                return None\n","            sparse_matrix = self.vectorizer.fit_transform(texts)\n","            dense_matrix = sparse_matrix.toarray()\n","            self.is_fitted = True\n","            return dense_matrix\n","        except Exception as e:\n","            print(f\"Error in fit_transform: {e}\")\n","            return None\n","\n","    def transform(self, texts: List[str]) -> np.ndarray:\n","        try:\n","            if not texts:\n","                return None\n","            if not self.is_fitted:\n","                raise ValueError(\"Vectorizer must be fitted first\")\n","            sparse_matrix = self.vectorizer.transform(texts)\n","            return sparse_matrix.toarray()\n","        except Exception as e:\n","            print(f\"Error in transform: {e}\")\n","            return None\n","\n","    def get_vectorizer_state(self):\n","        \"\"\"Get state of fitted vectorizer\"\"\"\n","        if not self.is_fitted:\n","            raise ValueError(\"Vectorizer must be fitted first\")\n","        return {\n","            'vocabulary_': self.vectorizer.vocabulary_,\n","            'idf_': self.vectorizer.idf_,\n","            'stop_words_': getattr(self.vectorizer, '_stop_words_', None)\n","        }\n","\n","    def set_vectorizer_state(self, state):\n","        \"\"\"Set state of vectorizer\"\"\"\n","        for key, value in state.items():\n","            setattr(self.vectorizer, key, value)\n","        self.is_fitted = True\n","\n","class ParallelDocumentProcessor:\n","    \"\"\"Enhanced main processor with improved vector store integration\"\"\"\n","    def __init__(self, drive_service, folder_id: str, save_dir: str):\n","        self.config = Config()\n","        self.logger = Logger(save_dir, self.config.LOG_LEVEL)\n","\n","        # Initialize Ray without dashboard\n","        ray.init(ignore_reinit_error=True, include_dashboard=False, log_to_driver=False)\n","\n","        self.drive_service = drive_service\n","        self.save_dir = Path(save_dir)\n","        self.folder_id = folder_id\n","\n","        # Create processing components\n","        self.processing_state = ProcessingState(save_dir, self.logger)\n","        self.progress = ProcessingProgress(save_dir, self.logger)\n","        self.progress.processing_state = self.processing_state\n","\n","        self.file_handler = FileHandler(\n","            drive_service,\n","            self.processing_state,\n","            self.config,\n","            self.logger\n","        )\n","\n","        # Initialize vector store with validation\n","        self.vector_store = VectorStore(save_dir, self.config, self.logger)\n","        self.verify_vector_store()\n","\n","        self.shutdown_handler = GracefulShutdown(self, self.logger)\n","\n","        # Initialize vectorizers\n","        self.vectorizers = [\n","            RayVectorizer.remote(self.config)\n","            for _ in range(self.config.VECTORIZE_WORKERS)\n","        ]\n","        self.is_fitted = False\n","\n","        # Set up status tracking\n","        self.last_status_time = time.time()\n","        self.start_time = time.time()\n","        self.status_interval = 300  # 5 minutes in seconds\n","\n","    def verify_vector_store(self):\n","        \"\"\"Verify vector store consistency with processing state\"\"\"\n","        try:\n","            # Get all processed file IDs from metadata\n","            vector_files = set(self.vector_store.metadata['vector_map'].keys())\n","            processed_files = self.processing_state.processed_files\n","\n","            # Check for any inconsistencies\n","            missing_vectors = processed_files - vector_files\n","            extra_vectors = vector_files - processed_files\n","\n","            if missing_vectors or extra_vectors:\n","                self.logger.warning(\n","                    f\"Vector store inconsistency detected:\\n\"\n","                    f\"â€¢ Files without vectors: {len(missing_vectors)}\\n\"\n","                    f\"â€¢ Vectors without files: {len(extra_vectors)}\"\n","                )\n","\n","                # Remove any extra vectors\n","                if extra_vectors:\n","                    self.vector_store.remove_vectors(list(extra_vectors))\n","\n","                # Reset processing state for files missing vectors\n","                for file_id in missing_vectors:\n","                    self.processing_state.processed_files.remove(file_id)\n","\n","                self.logger.info(\"Vector store verification complete. Inconsistencies resolved.\")\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error verifying vector store: {e}\")\n","\n","    def parallel_vectorize(self, texts: List[str]) -> Optional[np.ndarray]:\n","        \"\"\"Vectorize texts in parallel with improved error handling\"\"\"\n","        if not texts:\n","            return None\n","\n","        try:\n","            if not self.is_fitted:\n","                # Use first vectorizer to fit on all data\n","                first_vectorizer = self.vectorizers[0]\n","                result = ray.get(first_vectorizer.fit_transform.remote(texts))\n","                if result is None:\n","                    self.logger.error(\"Failed to fit vectorizer\")\n","                    return None\n","\n","                # Update state for all vectorizers\n","                state = ray.get(first_vectorizer.get_vectorizer_state.remote())\n","                update_futures = [\n","                    vectorizer.set_vectorizer_state.remote(state)\n","                    for vectorizer in self.vectorizers[1:]\n","                ]\n","                ray.get(update_futures)\n","                self.is_fitted = True\n","                return result\n","\n","            # Split texts for parallel processing\n","            n_workers = len(self.vectorizers)\n","            chunk_size = max(1, len(texts) // n_workers)\n","            chunks = [texts[i:i + chunk_size] for i in range(0, len(texts), chunk_size)]\n","\n","            # Process chunks in parallel\n","            futures = []\n","            for i, chunk in enumerate(chunks):\n","                if not chunk:  # Skip empty chunks\n","                    continue\n","                vectorizer_idx = i % len(self.vectorizers)\n","                futures.append(self.vectorizers[vectorizer_idx].transform.remote(chunk))\n","\n","            if not futures:\n","                self.logger.warning(\"No valid chunks to process\")\n","                return None\n","\n","            # Get results and handle errors\n","            results = []\n","            for future in futures:\n","                try:\n","                    result = ray.get(future)\n","                    if result is not None and result.size > 0:\n","                        results.append(result)\n","                except Exception as e:\n","                    self.logger.error(f\"Error processing chunk: {str(e)}\")\n","                    continue\n","\n","            if not results:\n","                return None\n","\n","            # Combine results with error checking\n","            try:\n","                combined = np.vstack([r for r in results if r is not None and r.size > 0])\n","                if combined.size == 0:\n","                    return None\n","                return combined\n","            except ValueError as e:\n","                self.logger.error(f\"Error combining results: {str(e)}\")\n","                return None\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error in parallel_vectorize: {str(e)}\")\n","            return None\n","\n","    def process_batch(self, batch: List[Dict[str, Any]]) -> bool:\n","        \"\"\"Process a single batch with improved vector store handling\"\"\"\n","        try:\n","            batch_info = {\n","                'start_time': datetime.now().isoformat(),\n","                'files': [f['id'] for f in batch],\n","                'start_index': len(self.processing_state.processed_files),\n","                'folder_id': self.folder_id\n","            }\n","            self.progress.current_batch = batch_info\n","\n","            # Process files\n","            processed_files = self.parallel_process_files(batch)\n","\n","            if processed_files:\n","                # Filter out any None or empty texts\n","                valid_files = [\n","                    f for f in processed_files\n","                    if f.get('text') and len(f['text'].strip()) > 0\n","                ]\n","\n","                if valid_files:\n","                    texts = [f['text'] for f in valid_files]\n","                    file_ids = [f['id'] for f in valid_files]\n","\n","                    # Attempt vectorization\n","                    vectors = self.parallel_vectorize(texts)\n","                    if vectors is not None and vectors.size > 0:\n","                        # Save vectors with validation\n","                        if self.vector_store.save_vectors(vectors, file_ids):\n","                            # Only mark as processed if vectors were saved successfully\n","                            for file_id in file_ids:\n","                                self.processing_state.mark_processed(file_id)\n","                        else:\n","                            self.logger.error(\"Failed to save vectors, batch will be retried\")\n","                            return False\n","                    else:\n","                        self.logger.warning(\n","                            f\"Vectorization failed for batch of {len(valid_files)} files\"\n","                        )\n","                else:\n","                    self.logger.warning(f\"No valid text content in batch of {len(processed_files)} files\")\n","\n","            # Update progress and status\n","            self.progress.current_batch = None\n","            self.progress.save_progress()\n","            self.log_status()\n","            return True\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error processing batch: {e}\")\n","            return False\n","\n","    def parallel_process_files(self, files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n","        \"\"\"Process files in parallel with chunking and error handling\"\"\"\n","        chunks = [\n","            files[i:i + self.config.CHUNK_SIZE]\n","            for i in range(0, len(files), self.config.CHUNK_SIZE)\n","        ]\n","\n","        chunk_futures = [\n","            process_file_chunk.remote(\n","                self.drive_service,\n","                chunk,\n","                self.processing_state,\n","                self.config,\n","                self.logger\n","            )\n","            for chunk in chunks\n","        ]\n","\n","        try:\n","            processed_chunks = ray.get(chunk_futures)\n","            return [item for chunk in processed_chunks for item in chunk if item]\n","        except Exception as e:\n","            self.logger.error(f\"Error in parallel processing: {e}\")\n","            return []\n","\n","    def log_status(self, force: bool = False):\n","        \"\"\"Log detailed status including vector store information\"\"\"\n","        current_time = time.time()\n","        if force or (current_time - self.last_status_time >= self.status_interval):\n","            elapsed = current_time - self.start_time\n","            processed = len(self.processing_state.processed_files)\n","            failed = len(self.processing_state.failed_files)\n","            skipped = sum(len(files) for files in self.progress.skipped_files.values())\n","            vectors = len(self.vector_store.metadata['vector_map'])\n","\n","            memory_percent = psutil.Process().memory_percent()\n","\n","            status_msg = (\n","                f\"Status after {elapsed:.1f}s:\\n\"\n","                f\"â€¢ Processed files: {processed:,}\\n\"\n","                f\"â€¢ Failed files: {failed:,}\\n\"\n","                f\"â€¢ Skipped files: {skipped:,}\\n\"\n","                f\"â€¢ Stored vectors: {vectors:,}\\n\"\n","                f\"â€¢ Memory usage: {memory_percent:.1f}%\"\n","            )\n","            self.logger.info(status_msg)\n","            self.last_status_time = current_time\n","\n","    def process_files(self):\n","        \"\"\"Main processing loop with improved resumability\"\"\"\n","        try:\n","            all_files = self.file_handler.list_files_recursive(self.folder_id)\n","\n","            # Filter already processed and skipped files\n","            remaining_files = []\n","            for f in all_files:\n","                if f['id'] in self.processing_state.processed_files:\n","                    self.progress.mark_skipped(f['id'], 'already_processed')\n","                    continue\n","\n","                mime_type = self.file_handler.get_mime_type(f['id'])\n","                if not mime_type or mime_type not in FileHandler.SUPPORTED_MIMETYPES:\n","                    self.progress.mark_skipped(f['id'], 'unsupported_type')\n","                    continue\n","\n","                remaining_files.append(f)\n","\n","            self.logger.info(f\"Found {len(remaining_files)} files to process\")\n","\n","            for i in range(0, len(remaining_files), self.config.BATCH_SIZE):\n","                if not self.shutdown_handler.should_continue():\n","                    self.logger.info(\"Shutdown requested, stopping processing\")\n","                    break\n","\n","                batch = remaining_files[i:i + self.config.BATCH_SIZE]\n","                if not self.process_batch(batch):\n","                    self.logger.error(\"Batch processing failed, stopping\")\n","                    break\n","\n","                if i % self.config.CHECKPOINT_INTERVAL == 0:\n","                    self.processing_state.save_state()\n","                    self.log_status(force=True)\n","                    self.logger.info(\n","                        f\"Checkpoint: {i + len(batch)}/{len(remaining_files)} files processed\"\n","                    )\n","\n","        except Exception as e:\n","            self.logger.error(f\"Error in process_files: {e}\")\n","            raise\n","\n","        finally:\n","            self.progress.save_progress(force=True)\n","            self.log_status(force=True)\n","\n","    def run(self):\n","        \"\"\"Main execution method with improved cleanup\"\"\"\n","        try:\n","            self.logger.info(\"Starting document processing\")\n","            self.process_files()\n","\n","            # Verify final state\n","            self.verify_vector_store()\n","            self.log_status(force=True)\n","\n","            self.logger.info(\"Processing completed successfully\")\n","\n","        except Exception as e:\n","            self.logger.error(f\"Processing failed: {e}\")\n","            raise\n","\n","        finally:\n","            self.shutdown_handler.cleanup()\n","\n","def create_drive_service():\n","    \"\"\"Create and authenticate Google Drive service\"\"\"\n","    try:\n","        auth.authenticate_user()\n","        drive.mount('/content/drive')\n","        service = build('drive', 'v3')\n","        return service\n","    except Exception as e:\n","        print(f\"Error creating Drive service: {e}\")\n","        sys.exit(1)\n","\n","def main():\n","    \"\"\"Main execution function for Google Colab environment with resume capability and auto-restart on exception\"\"\"\n","    while True:\n","        try:\n","            # Check if any processing state exists\n","            state_file = Path(SAVE_DIR) / 'processing_state.pkl'\n","            progress_file = Path(SAVE_DIR) / 'progress.json'\n","            has_previous_state = state_file.exists() or progress_file.exists()\n","\n","            # Increase recursion limit\n","            sys.setrecursionlimit(10000)\n","\n","            # Create save directory\n","            os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","            # Create service\n","            drive_service = create_drive_service()\n","\n","            # Initialize processor with existing state\n","            processor = ParallelDocumentProcessor(\n","                drive_service=drive_service,\n","                folder_id=FOLDER_ID,\n","                save_dir=SAVE_DIR\n","            )\n","\n","            if has_previous_state:\n","                processor.logger.info(\"Resuming previously interrupted processing...\")\n","                processor.logger.info(f\"Found {len(processor.processing_state.processed_files)} previously processed files\")\n","                processor.logger.info(f\"Found {len(processor.processing_state.failed_files)} previously failed files\")\n","\n","                # Log skipped files if available\n","                if processor.progress.skipped_files:\n","                    total_skipped = sum(len(files) for files in processor.progress.skipped_files.values())\n","                    processor.logger.info(f\"Found {total_skipped} previously skipped files\")\n","\n","            # Run processor\n","            processor.run()\n","\n","            processor.logger.info(\"Processing completed successfully\")\n","            return 0\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nGracefully shutting down...\")\n","            return 130  # Standard Unix practice for Ctrl+C\n","        except Exception as e:\n","            print(f\"Processing failed: {str(e)}\")\n","            print(\"Restarting in 5 seconds...\")\n","            time.sleep(5)  # Wait for 5 seconds before restarting\n","\n","if __name__ == '__main__':\n","    while True:\n","        exit_code = main()\n","        if exit_code != 1:  # Assuming 1 signifies an exception that should trigger a restart\n","            sys.exit(exit_code)"]},{"cell_type":"markdown","metadata":{"id":"Mjx_B_Yx2Pzm"},"source":["## Check the creation of files from previous step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15NtNv1_OCxv","cellView":"form"},"outputs":[],"source":["# @title\n","from google.colab import drive\n","import os\n","from pathlib import Path\n","import json\n","import faiss\n","from typing import Optional, Dict, Any\n","import numpy as np\n","\n","def check_vector_db(save_dir: str = SAVE_DIR) -> Optional[Dict[str, Any]]:\n","    \"\"\"\n","    Check the existence and integrity of the vector database and processing state.\n","\n","    Args:\n","        save_dir: Directory where vector database and related files are stored (defaults to SAVE_DIR from env)\n","\n","    Returns:\n","        Optional[Dict]: Dictionary containing database stats if files exist, None otherwise\n","    \"\"\"\n","    try:\n","        # Mount Google Drive if not already mounted\n","        if not os.path.exists('/content/drive'):\n","            drive.mount('/content/drive')\n","\n","        # Convert save_dir to Path object\n","        save_path = Path(str(save_dir))\n","\n","        # Ensure directory exists\n","        if not save_path.exists():\n","            print(f\"Directory does not exist: {save_path}\")\n","            return None\n","\n","        # Define essential files for FAISS vector database\n","        required_files = {\n","            'faiss_index': save_path / 'faiss_index.index',\n","            'metadata': save_path / 'metadata.json',\n","            'processing_state': save_path / 'processing_state.pkl'\n","        }\n","\n","        # Check essential files exist\n","        missing_files = []\n","        for name, path in required_files.items():\n","            if not path.exists():\n","                missing_files.append(name)\n","\n","        if missing_files:\n","            print(f\"\\nMissing essential files in {save_path}:\")\n","            for file in missing_files:\n","                print(f\"  â€¢ {file}\")\n","            return None\n","\n","        # Load files and perform integrity check\n","        try:\n","            # Load FAISS index\n","            index = faiss.read_index(str(required_files['faiss_index']))\n","\n","            # Load metadata\n","            with open(required_files['metadata']) as f:\n","                metadata = json.load(f)\n","\n","            # Collect database statistics\n","            stats = {\n","                'vector_count': index.ntotal,\n","                'vector_dimension': index.d,\n","                'indexed_files': len(metadata['vector_map']),\n","                'last_update': metadata.get('last_update'),\n","                'storage_location': str(save_path),\n","                'memory_usage': {\n","                    'index_size': os.path.getsize(required_files['faiss_index']) / (1024 * 1024),  # MB\n","                    'metadata_size': os.path.getsize(required_files['metadata']) / (1024 * 1024),  # MB\n","                }\n","            }\n","\n","            # Print detailed report\n","            print(\"\\nVector Database Status:\")\n","            print(\"-\" * 50)\n","            print(f\"â€¢ Storage Location: {stats['storage_location']}\")\n","            print(f\"â€¢ Total vectors: {stats['vector_count']}\")\n","            print(f\"â€¢ Vector dimensionality: {stats['vector_dimension']}\")\n","            print(f\"â€¢ Indexed files: {stats['indexed_files']}\")\n","            if stats.get('last_update'):\n","                print(f\"â€¢ Last update: {stats['last_update']}\")\n","\n","            print(\"\\nStorage Usage:\")\n","            print(f\"â€¢ FAISS index: {stats['memory_usage']['index_size']:.2f} MB\")\n","            print(f\"â€¢ Metadata: {stats['memory_usage']['metadata_size']:.2f} MB\")\n","\n","            # Basic integrity check\n","            if stats['vector_count'] != stats['indexed_files']:\n","                print(\"\\nWARNING: Number of vectors doesn't match metadata entries!\")\n","\n","            return stats\n","\n","        except Exception as e:\n","            print(f\"Error loading database files: {str(e)}\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"Error: {str(e)}\")\n","        return None\n","\n","if __name__ == '__main__':\n","    print(f\"Checking vector database in: {SAVE_DIR}\")\n","    stats = check_vector_db()\n","    if stats:\n","        print(\"\\nDatabase check completed successfully.\")"]},{"cell_type":"markdown","metadata":{"id":"Py7dNgNP2lP2"},"source":["## Interaction between xAI and Vector Knowledgebase"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rievyGcQ8QgM","cellView":"form"},"outputs":[],"source":["# @title\n","\"\"\"\n","Enhanced Chat Interface with FAISS Vector Database Integration\n","\"\"\"\n","\n","import requests\n","import json\n","from IPython.display import display, clear_output, Markdown, HTML\n","import ipywidgets as widgets\n","from datetime import datetime\n","import time\n","import os\n","from google.colab import drive\n","import faiss\n","import numpy as np\n","from pathlib import Path\n","from typing import List, Dict, Any, Optional, Tuple\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","class VectorDB:\n","    \"\"\"FAISS vector database manager\"\"\"\n","    def __init__(self, save_dir: str):\n","        self.save_dir = Path(str(save_dir))\n","        self.index_path = self.save_dir / 'faiss_index.index'\n","        self.metadata_path = self.save_dir / 'metadata.json'\n","        self.dimension = 512  # Match the dimension used in processing\n","        self._load_state()\n","\n","    def _load_state(self):\n","        \"\"\"Load or create index and metadata with validation\"\"\"\n","        try:\n","            if self.index_path.exists() and self.metadata_path.exists():\n","                self.index = faiss.read_index(str(self.index_path))\n","                with open(self.metadata_path) as f:\n","                    self.metadata = json.load(f)\n","                print(f\"Loaded FAISS index with {self.index.ntotal:,} vectors\")\n","            else:\n","                print(\"No existing vector database found\")\n","                self.index = None\n","                self.metadata = None\n","        except Exception as e:\n","            print(f\"Error loading vector database: {e}\")\n","            self.index = None\n","            self.metadata = None\n","\n","    def find_similar_documents(self, query_vector: np.ndarray, k: int = 3) -> List[Tuple[str, float]]:\n","        \"\"\"Find k most similar documents using FAISS\"\"\"\n","        if self.index is None or self.metadata is None:\n","            return []\n","\n","        try:\n","            # Search for similar vectors\n","            distances, indices = self.index.search(query_vector.reshape(1, -1), min(k, self.index.ntotal))\n","\n","            results = []\n","            for idx, dist in zip(indices[0], distances[0]):\n","                # Find corresponding document ID\n","                for file_id, meta in self.metadata['vector_map'].items():\n","                    if meta['index'] == idx:\n","                        similarity = 1 - (dist / 2)  # Convert L2 distance to similarity score\n","                        results.append((file_id, similarity))\n","                        break\n","\n","            return sorted(results, key=lambda x: x[1], reverse=True)\n","\n","        except Exception as e:\n","            print(f\"Error in similarity search: {e}\")\n","            return []\n","\n","    def get_document_vector(self, file_id: str) -> Optional[np.ndarray]:\n","        \"\"\"Get vector for a specific document\"\"\"\n","        if self.index is None or self.metadata is None:\n","            return None\n","\n","        try:\n","            meta = self.metadata['vector_map'].get(file_id)\n","            if not meta:\n","                return None\n","            idx = meta['index']\n","            if idx >= self.index.ntotal:\n","                return None\n","            return self.index.reconstruct(idx)\n","        except Exception as e:\n","            print(f\"Error retrieving vector: {e}\")\n","            return None\n","\n","class ChatLogger:\n","    \"\"\"Handles chat logging to markdown files\"\"\"\n","    def __init__(self, save_dir: str):\n","        self.save_dir = Path(str(save_dir))\n","        self.chat_dir = self.save_dir / 'chat_logs'\n","        self.chat_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Create or load current session file\n","        self.session_file = self.chat_dir / f\"chat_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n","        self.session_file.touch()\n","\n","    def log_interaction(self, query: str, response: str, references: str = \"\"):\n","        \"\"\"Log a single interaction to both session and individual files\"\"\"\n","        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        interaction = (\n","            f\"## {timestamp}\\n\\n\"\n","            f\"**Question:** {query}\\n\\n\"\n","            f\"**Response:**\\n{response}\\n\"\n","            f\"{references}\\n\\n\"\n","            f\"---\\n\\n\"\n","        )\n","\n","        # Prepend to session file\n","        current_content = self.session_file.read_text() if self.session_file.exists() else \"\"\n","        self.session_file.write_text(interaction + current_content)\n","\n","        # Create individual log file for this interaction\n","        individual_file = self.chat_dir / f\"interaction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n","        individual_file.write_text(interaction)\n","\n","    def get_recent_interactions(self, limit: int = 5) -> str:\n","        \"\"\"Get recent interactions from session file\"\"\"\n","        if not self.session_file.exists():\n","            return \"\"\n","        content = self.session_file.read_text()\n","        interactions = content.split(\"---\\n\\n\")[:limit]\n","        return \"---\\n\\n\".join(interactions)\n","\n","class VectorSearchMetrics:\n","    \"\"\"Track and analyze vector search performance\"\"\"\n","    def __init__(self):\n","        self.total_searches = 0\n","        self.successful_searches = 0\n","        self.average_similarity = 0.0\n","        self.search_times = []\n","        self.error_counts: Dict[str, int] = {}\n","\n","    def log_search(self, duration: float, results: List[Tuple[str, float]], error: Optional[str] = None):\n","        \"\"\"Log a search operation\"\"\"\n","        self.total_searches += 1\n","\n","        if error:\n","            self.error_counts[error] = self.error_counts.get(error, 0) + 1\n","        else:\n","            self.successful_searches += 1\n","            self.search_times.append(duration)\n","\n","            if results:\n","                similarities = [sim for _, sim in results]\n","                self.average_similarity = (\n","                    (self.average_similarity * (self.successful_searches - 1) +\n","                     sum(similarities) / len(similarities)) / self.successful_searches\n","                )\n","\n","    def get_stats(self) -> Dict[str, Any]:\n","        \"\"\"Get current search statistics\"\"\"\n","        return {\n","            'total_searches': self.total_searches,\n","            'successful_searches': self.successful_searches,\n","            'success_rate': self.successful_searches / max(1, self.total_searches),\n","            'average_similarity': self.average_similarity,\n","            'average_search_time': sum(self.search_times) / max(1, len(self.search_times)),\n","            'error_distribution': self.error_counts\n","        }\n","\n","class ChatInterface:\n","    \"\"\"Enhanced chat interface with improved FAISS vector search\"\"\"\n","    def __init__(self, save_dir: str = SAVE_DIR):\n","        self.save_dir = Path(str(save_dir))\n","        self.vector_db = VectorDB(save_dir)\n","        self.logger = ChatLogger(save_dir)\n","        self.metrics = VectorSearchMetrics()\n","        self.setup_interface()\n","\n","    def setup_interface(self):\n","        \"\"\"Initialize the chat interface with status display\"\"\"\n","        # Create interface elements\n","        self.status_area = widgets.Output()\n","        self.chat_area = widgets.Output()\n","        self.input_area = widgets.Text(\n","            placeholder='Ask a question...',\n","            layout=widgets.Layout(width='80%')\n","        )\n","        self.send_button = widgets.Button(\n","            description=\"Send\",\n","            button_style='info',\n","            layout=widgets.Layout(width='10%')\n","        )\n","        self.clear_button = widgets.Button(\n","            description=\"Clear\",\n","            button_style='warning',\n","            layout=widgets.Layout(width='10%')\n","        )\n","\n","        # Show database status\n","        with self.status_area:\n","            if self.vector_db.index is not None:\n","                stats = self.get_vector_stats()\n","                display(HTML(\n","                    f\"<b>Vector Database Status:</b><br>\"\n","                    f\"â€¢ {stats['total_vectors']:,} vectors loaded<br>\"\n","                    f\"â€¢ Vector dimension: {stats['dimension']}<br>\"\n","                    f\"â€¢ Documents indexed: {stats['indexed_documents']:,}<br>\"\n","                    f\"â€¢ Index size: {stats['memory_usage']['index_size']:.2f} MB\"\n","                ))\n","                recent = self.logger.get_recent_interactions(3)\n","                if recent:\n","                    display(Markdown(\"\\n**Recent Interactions:**\\n\" + recent))\n","            else:\n","                display(HTML(\"<b>Warning:</b> Vector database not loaded\"))\n","\n","        # Set up callbacks\n","        self.send_button.on_click(self.handle_send)\n","        self.clear_button.on_click(self.handle_clear)\n","        self.input_area.on_submit(self.handle_submit)\n","\n","        # Layout\n","        button_box = widgets.HBox([self.send_button, self.clear_button])\n","        input_box = widgets.VBox([\n","            widgets.Label('Enter your question:'),\n","            widgets.HBox([self.input_area, button_box])\n","        ])\n","        display(widgets.VBox([self.status_area, input_box, self.chat_area]))\n","\n","    def vectorize_query(self, query: str) -> Optional[np.ndarray]:\n","        \"\"\"Convert query to vector matching index dimension\"\"\"\n","        try:\n","            if not self.vector_db.index:\n","                return None\n","\n","            # Get example vector for dimension\n","            example_vector = self.vector_db.index.reconstruct(0)\n","            dimension = len(example_vector)\n","\n","            # Configure vectorizer\n","            vectorizer = TfidfVectorizer(\n","                max_features=dimension,\n","                strip_accents='unicode',\n","                analyzer='word',\n","                stop_words='english'\n","            )\n","\n","            # Create vector\n","            vector = vectorizer.fit_transform([query]).toarray()[0]\n","            vector = np.pad(vector, (0, max(0, dimension - len(vector))))[:dimension]\n","\n","            # Normalize\n","            norm = np.linalg.norm(vector)\n","            if norm > 0:\n","                vector = vector / norm\n","\n","            return vector\n","\n","        except Exception as e:\n","            print(f\"Error vectorizing query: {e}\")\n","            return None\n","\n","    def find_relevant_context(self, query: str) -> List[Tuple[str, float]]:\n","        \"\"\"Find relevant documents using normalized vectors\"\"\"\n","        if self.vector_db.index is None:\n","            return []\n","\n","        try:\n","            query_vector = self.vectorize_query(query)\n","            if query_vector is None:\n","                return []\n","\n","            similar_docs = self.vector_db.find_similar_documents(query_vector, k=3)\n","            return similar_docs\n","\n","        except Exception as e:\n","            print(f\"Error finding relevant context: {e}\")\n","            return []\n","\n","    def format_context(self, similar_docs: List[Tuple[str, float]]) -> str:\n","        \"\"\"Format context with document information\"\"\"\n","        if not similar_docs:\n","            return \"No relevant context found.\"\n","\n","        context_parts = []\n","        for doc_id, similarity in similar_docs:\n","            vector = self.vector_db.get_document_vector(doc_id)\n","            vector_info = (\n","                f\"(norm: {np.linalg.norm(vector):.2f})\"\n","                if vector is not None else \"\"\n","            )\n","            context_parts.append(\n","                f\"Document {doc_id} \"\n","                f\"(similarity: {similarity:.2f}) {vector_info}\"\n","            )\n","\n","        return \"Related documents:\\n\" + \"\\n\".join(f\"â€¢ {part}\" for part in context_parts)\n","\n","    def get_grok_response(self, query: str, context: str) -> str:\n","        \"\"\"Get response from Grok API\"\"\"\n","        payload = {\n","            \"messages\": [\n","                {\n","                    \"role\": \"system\",\n","                    \"content\": \"You are a helpful assistant with access to a knowledge base. \"\n","                              \"Use the provided context when relevant, but you can also draw \"\n","                              \"on your general knowledge when appropriate.\"\n","                },\n","                {\n","                    \"role\": \"user\",\n","                    \"content\": f\"{query}\\n\\nContext: {context}\"\n","                }\n","            ],\n","            \"model\": MODEL,\n","            \"stream\": False,\n","            \"temperature\": float(TEMP)\n","        }\n","\n","        headers = {\n","            \"Authorization\": f\"Bearer {grok_api_key}\",\n","            \"Content-Type\": \"application/json\"\n","        }\n","\n","        try:\n","            response = requests.post(\n","                \"https://api.x.ai/v1/chat/completions\",\n","                headers=headers,\n","                json=payload,\n","                timeout=60\n","            )\n","\n","            if response.status_code == 200:\n","                return response.json()['choices'][0]['message']['content']\n","            else:\n","                return f\"Error: API returned status code {response.status_code}\"\n","\n","        except Exception as e:\n","            return f\"Error: {str(e)}\"\n","\n","    def format_references(self, docs: List[Tuple[str, float]]) -> str:\n","        \"\"\"Format reference links with metadata\"\"\"\n","        if not docs:\n","            return \"\"\n","\n","        refs = []\n","        for doc_id, similarity in docs:\n","            url = f\"https://drive.google.com/file/d/{doc_id}/view?usp=sharing\"\n","            vector = self.vector_db.get_document_vector(doc_id)\n","            vector_info = f\"(norm: {np.linalg.norm(vector):.2f})\" if vector is not None else \"\"\n","            refs.append(f\"[Document {doc_id[:8]}...]({url}) ({similarity:.2f}) {vector_info}\")\n","\n","        return \"\\n\\n---\\n**References:**\\n\\n\" + \"\\n\".join(f\"â€¢ {ref}\" for ref in refs)\n","\n","    def send_message(self, query: str):\n","        \"\"\"Process and send message\"\"\"\n","        with self.chat_area:\n","            clear_output(wait=True)\n","\n","        similar_docs = self.find_relevant_context(query)\n","        context = self.format_context(similar_docs)\n","        response = self.get_grok_response(query, context)\n","        references = self.format_references(similar_docs)\n","\n","        self.logger.log_interaction(query, response, references)\n","\n","        with self.chat_area:\n","            display(Markdown(self.logger.get_recent_interactions(1)))\n","\n","    def handle_submit(self, widget):\n","        \"\"\"Handle enter key\"\"\"\n","        self.handle_send(None)\n","\n","    def handle_send(self, b):\n","        \"\"\"Handle send button\"\"\"\n","        query = self.input_area.value.strip()\n","        if query:\n","            self.input_area.value = ''\n","            self.send_message(query)\n","\n","    def handle_clear(self, b):\n","        \"\"\"Handle clear button\"\"\"\n","        with self.chat_area:\n","            clear_output()\n","        self.input_area.value = ''\n","\n","    def get_vector_stats(self) -> Dict[str, Any]:\n","        \"\"\"Get vector database statistics\"\"\"\n","        return {\n","            'total_vectors': self.vector_db.index.ntotal,\n","            'dimension': self.vector_db.dimension,\n","            'indexed_documents': len(self.vector_db.metadata['vector_map']),\n","            'memory_usage': {\n","                'index_size': os.path.getsize(self.vector_db.index_path) / (1024 * 1024),\n","                'metadata_size': os.path.getsize(self.vector_db.metadata_path) / (1024 * 1024)\n","            }\n","        }\n","\n","def main():\n","    \"\"\"Initialize and start the chat interface\"\"\"\n","    print(f\"Initializing chat interface with vector database from: {SAVE_DIR}\")\n","    ChatInterface()\n","\n","if __name__ == '__main__':\n","    try:\n","        # Mount Google Drive if needed\n","        if not os.path.exists('/content/drive'):\n","            drive.mount('/content/drive')\n","\n","        print(f\"\\nInitializing chat interface with vector database from: {SAVE_DIR}\")\n","\n","        # Create interface\n","        chat = ChatInterface()\n","\n","        # Show start message\n","        print(\"\\nChat interface ready. Type your questions in the input box above.\")\n","        print(f\"Using model: {MODEL}\")\n","        print(f\"Temperature: {TEMP}\\n\")\n","\n","    except Exception as e:\n","        print(f\"Error initializing chat interface: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvXt9qFWbh5M"},"outputs":[],"source":[]}],"metadata":{"colab":{"cell_execution_strategy":"setup","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPIFwTyanzTixsb609mC6X+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
